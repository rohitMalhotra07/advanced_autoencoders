[
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "Exported source\nimport os\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchvision.transforms as transforms\n\n\n\nsource\n\nseed_everything\n\n seed_everything (seed=42)\n\n\n\nExported source\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONASSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n\n\nsource\n\n\nmake_images_dataframe\n\n make_images_dataframe (cnfg)\n\nMakes a dataframe of all images in the image dir\n\n\nExported source\ndef make_images_dataframe(cnfg) -&gt; pd.DataFrame:\n    \"\"\"\n    Makes a dataframe of all images in the image dir\n    \"\"\"\n    dir_images = Path(cnfg.IMG_DIR)\n    all_image_names = [f.stem for f in dir_images.iterdir() if f.suffix != \"\"]\n    all_image_suffix = [f.suffix for f in dir_images.iterdir() if f.suffix != \"\"]\n    df_all = pd.DataFrame(\n        {\"image_name\": all_image_names, \"image_sufix\": all_image_suffix}\n    )\n\n    df_all[\"file_image\"] = df_all.apply(\n        lambda x: str(dir_images / f\"{x.image_name}{x.image_sufix}\"), axis=1\n    )\n\n    # df_all = df_all.sample(100).reset_index(drop=True)\n\n    return df_all\n\n\n\nsource\n\n\nget_train_transforms\n\n get_train_transforms (cnfg)\n\n\n\nExported source\ndef get_train_transforms(cnfg):\n    transform_train = transforms.Compose(\n        [\n            # transforms.RandomCrop(cnfg.img_size, padding=4),\n            transforms.Resize((cnfg.img_size, cnfg.img_size)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        ]\n    )\n\n    return transform_train\n\n\n\nsource\n\n\nget_test_transforms\n\n get_test_transforms (cnfg)\n\n\n\nExported source\ndef get_test_transforms(cnfg):\n    transform_test = transforms.Compose(\n        [\n            transforms.Resize((cnfg.img_size, cnfg.img_size)),\n            transforms.ToTensor(),\n            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        ]\n    )\n    return transform_test\n\n\n\nsource\n\n\nget_2d_sincos_pos_embed\n\n get_2d_sincos_pos_embed (embed_dim, grid_size, cls_token=False)\n\ngrid_size: int of the grid height and width return: pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n\n\nExported source\ndef get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=np.float32)\n    grid_w = np.arange(grid_size, dtype=np.float32)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size, grid_size])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n\n\n\nsource\n\n\nget_2d_sincos_pos_embed\n\n get_2d_sincos_pos_embed (embed_dim, grid_size, cls_token=False)\n\ngrid_size: int of the grid height and width return: pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n\n\nExported source\ndef get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=np.float32)\n    grid_w = np.arange(grid_size, dtype=np.float32)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size, grid_size])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n\n\n\nsource\n\n\nget_2d_sincos_pos_embed_from_grid\n\n get_2d_sincos_pos_embed_from_grid (embed_dim, grid)\n\n\n\nExported source\ndef get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n    assert embed_dim % 2 == 0\n\n    # use half of dimensions to encode grid_h\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n\n    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n    return emb\n\n\n\nsource\n\n\nget_1d_sincos_pos_embed_from_grid\n\n get_1d_sincos_pos_embed_from_grid (embed_dim, pos)\n\nembed_dim: output dimension for each position pos: a list of positions to be encoded: size (M,) out: (M, D)\n\n\nExported source\ndef get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=np.float_)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum(\"m,d-&gt;md\", pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n    return emb\n\n\n\nsource\n\n\nget_1d_sincos_pos_embed_from_grid\n\n get_1d_sincos_pos_embed_from_grid (embed_dim, pos)\n\nembed_dim: output dimension for each position pos: a list of positions to be encoded: size (M,) out: (M, D)\n\n\nExported source\ndef get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=np.float_)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum(\"m,d-&gt;md\", pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n    return emb\n\n\n\nsource\n\n\ninterpolate_pos_embed\n\n interpolate_pos_embed (model, checkpoint_model)\n\n\n\nExported source\ndef interpolate_pos_embed(model, checkpoint_model):\n    if \"pos_embed\" in checkpoint_model:\n        pos_embed_checkpoint = checkpoint_model[\"pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.patch_embed.num_patches\n        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\n                \"Position interpolate from %dx%d to %dx%d\"\n\n            )\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(\n                -1, orig_size, orig_size, embedding_size\n            ).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(\n                pos_tokens,\n                size=(new_size, new_size),\n                mode=\"bicubic\",\n                align_corners=False,\n            )\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            checkpoint_model[\"pos_embed\"] = new_pos_embed",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "advanced_autoencoders",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "advanced_autoencoders"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "advanced_autoencoders",
    "section": "Install",
    "text": "Install\npip install advanced_autoencoders",
    "crumbs": [
      "advanced_autoencoders"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "advanced_autoencoders",
    "section": "How to use",
    "text": "How to use",
    "crumbs": [
      "advanced_autoencoders"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "foo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "train.html",
    "href": "train.html",
    "title": "Train model",
    "section": "",
    "text": "from advanced_autoencoders.training_pipelines import (\n    train_pipeline_mae,\n    train_pipeline_vqvae,\n)\n\n\n# train_pipeline_mae()\n\n\ntrain_pipeline_vqvae()\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nMissing logger folder: ../training_logs/dalle_vq_vae_32_32_2_v2/lightning_logs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name  | Type    | Params\n----------------------------------\n0 | model | MyVQVAE | 2.6 M \n----------------------------------\n2.6 M     Trainable params\n0         Non-trainable params\n2.6 M     Total params\n10.327    Total estimated model params size (MB)\n\n\nMaking Image DF from DIR DONE! TOTAL IMAGES:64467\nTRAIN VAL SPLIT DONE! TOTAL IMAGES TRAIN:51573, VAL:12894\nValidity check done!",
    "crumbs": [
      "Train model"
    ]
  },
  {
    "objectID": "trainers.html",
    "href": "trainers.html",
    "title": "Define Lightning based trainers",
    "section": "",
    "text": "Exported source\nimport os\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pytorch_lightning as pl\nimport torch\nimport torch.optim as optim\nimport torchvision\nfrom colorama import Fore, Style\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom torch.utils.data import DataLoader\n\nfrom advanced_autoencoders.dataset import MyImageDataset\nfrom advanced_autoencoders.models import get_mae_model, get_vae_model\nfrom advanced_autoencoders.utils import get_test_transforms, get_train_transforms\n\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL\nwarnings.filterwarnings(\"ignore\")\n\n\n\nsource\n\ngenerate_and_save_images_vq\n\n generate_and_save_images_vq (dvae_model, epoch, test_sample, best_loss,\n                              cnfg, figsize=(20, 15))\n\n\n\nExported source\ndef generate_and_save_images_vq(\n    dvae_model, epoch, test_sample, best_loss, cnfg, figsize=(20, 15)\n):\n    f, axarr = plt.subplots(1, 2, figsize=figsize)\n    img1 = torchvision.utils.make_grid(test_sample.cpu()).permute(1, 2, 0).numpy()\n    # axarr[0].imshow(img)\n\n    outputs = dvae_model(test_sample)\n    predictions = outputs.detach().cpu()\n    img2 = torchvision.utils.make_grid(predictions).permute(1, 2, 0).numpy()\n\n    axarr[0].imshow(img1)\n    axarr[1].imshow(img2)\n    axarr[0].set_title(\"Orignal\")\n    axarr[1].set_title(\"Reconstruction\")\n\n    if not os.path.exists(cnfg.TRAINING_RECON_IMG_DIR):\n        os.makedirs(cnfg.TRAINING_RECON_IMG_DIR)\n    plt.savefig(f\"{cnfg.TRAINING_RECON_IMG_DIR}image_{epoch}_{best_loss}.png\")\n\n\n\nsource\n\n\ngenerate_and_save_images_mae\n\n generate_and_save_images_mae (model, epoch, test_sample, best_loss, cnfg,\n                               figsize=(20, 15))\n\n\n\nExported source\ndef generate_and_save_images_mae(\n    model, epoch, test_sample, best_loss, cnfg, figsize=(20, 15)\n):\n    f, axarr = plt.subplots(1, 2, figsize=figsize)\n    img1 = torchvision.utils.make_grid(test_sample.cpu()).permute(1, 2, 0).numpy()\n\n    loss, pred, mask = model(test_sample)\n    predictions = model.unpatchify(pred)\n    predictions = predictions.detach().cpu()\n    img2 = torchvision.utils.make_grid(predictions).permute(1, 2, 0).numpy()\n\n    axarr[0].imshow(img1)\n    axarr[1].imshow(img2)\n    axarr[0].set_title(\"Orignal\")\n    axarr[1].set_title(\"Reconstruction\")\n\n    if not os.path.exists(cnfg.TRAINING_RECON_IMG_DIR):\n        os.makedirs(cnfg.TRAINING_RECON_IMG_DIR)\n    plt.savefig(f\"{cnfg.TRAINING_RECON_IMG_DIR}image_{epoch}_{best_loss}.png\")\n\n\n\nsource\n\n\nPlModelVQVAE\n\n PlModelVQVAE (cnfg, df_train, df_val, test_sample)\n\nHooks to be used in LightningModule.\n\n\nExported source\nclass PlModelVQVAE(pl.LightningModule):\n    def __init__(self, cnfg, df_train, df_val, test_sample):\n        super().__init__()\n\n        self.model = get_vae_model(cnfg)\n        # self.model.load_state_dict(torch.load(\"./\" + best_model_name + \".bin\"))\n\n        self.l_rate = cnfg.lr\n        self.batch_size = cnfg.bs\n        self.best_loss = 99999\n        self.epoch_train_loss = None\n        self.epoch_val_loss = None\n        self.epoch_val_rec_loss = None\n        self.validity_check_happened = False\n        self.cnfg = cnfg\n        self.test_sample = test_sample\n\n        self.df_train = df_train\n        self.df_val = df_val\n\n        self.validation_step_outs = []\n        self.train_step_outs = []\n\n    def forward(\n        self, inpt, return_loss=True, return_recons=False, return_encoded=False\n    ):\n        return self.model(\n            inpt,\n            return_loss=True,\n            return_recons=return_recons,\n            return_encoded=return_encoded,\n        )\n\n    def train_dataloader(self):\n        # train\n        train_dataset = MyImageDataset(\n            self.df_train, augmentations=get_train_transforms(self.cnfg)\n        )\n        train_dl = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n        return train_dl\n\n    def val_dataloader(self):\n        # valid\n        valid_dataset = MyImageDataset(\n            self.df_val, augmentations=get_train_transforms(self.cnfg)\n        )\n        valid_dl = DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False)\n        return valid_dl\n\n    def training_step(self, batch, batch_idx):\n        loss, _ = self.forward(batch)\n\n        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.train_step_outs.append({\"loss\": loss})\n        return loss\n\n    def on_train_epoch_end(self):\n        outputs = [k[\"loss\"].item() for k in self.train_step_outs]\n        self.epoch_train_loss = sum(outputs) / len(outputs)\n\n        self.train_step_outs = []\n\n        return\n\n    def validation_step(self, batch, batch_idx):\n        with torch.no_grad():\n            val_loss, val_recon_loss = self.forward(batch)\n\n            self.log(\"val_loss\", val_loss, on_epoch=True, prog_bar=True, logger=True)\n            self.log(\n                \"val_recon_loss\",\n                val_recon_loss,\n                on_epoch=True,\n                prog_bar=True,\n                logger=True,\n            )\n\n        self.validation_step_outs.append(\n            {\"val_loss\": val_loss, \"val_recon_loss\": val_recon_loss}\n        )\n        return {\"val_loss\": val_loss, \"val_recon_loss\": val_recon_loss}\n\n    def on_validation_epoch_end(self):\n        if self.validity_check_happened:\n            val_losses = [k[\"val_loss\"].item() for k in self.validation_step_outs]\n            val_recon_losses = [\n                k[\"val_recon_loss\"].item() for k in self.validation_step_outs\n            ]\n            self.epoch_val_loss = sum(val_losses) / len(val_losses)\n            self.epoch_val_rec_loss = sum(val_recon_losses) / len(val_recon_losses)\n\n            print(\n                f\"Epoch:{self.current_epoch} |Train Loss:{self.epoch_train_loss}|Valid Loss:{self.epoch_val_loss}|Rec Loss:{self.epoch_val_rec_loss}\"\n            )\n            if self.epoch_val_rec_loss &lt;= self.best_loss:\n                print(\n                    f\"{g_}Rec Loss Decreased from {self.best_loss} to {self.epoch_val_rec_loss}{sr_}\"\n                )\n\n                self.best_loss = self.epoch_val_rec_loss\n                torch.save(\n                    self.model.state_dict(),\n                    f\"{self.cnfg.MODELS_DIR}{self.cnfg.model_name}.bin\",\n                )\n                with torch.no_grad():\n                    generate_and_save_images_vq(\n                        self.model,\n                        self.current_epoch,\n                        self.test_sample,\n                        self.best_loss,\n                        self.cnfg,\n                    )\n        else:\n            self.validity_check_happened = True\n            print(\"Validity check done!\")\n\n        self.validation_step_outs = []\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.l_rate)\n\n\n\nsource\n\n\nPlModelMAE\n\n PlModelMAE (cnfg, df_train, df_val, test_sample)\n\nHooks to be used in LightningModule.\n\n\nExported source\nclass PlModelMAE(pl.LightningModule):\n    def __init__(self, cnfg, df_train, df_val, test_sample):\n        super().__init__()\n\n        self.model = get_mae_model(cnfg)\n        if cnfg.prev_model_name is not None:\n            self.model.load_state_dict(torch.load(\"./\" + cnfg.prev_model_name + \".bin\"))\n            print(\"Previous Model Loaded\")\n\n        self.best_loss = 99999\n        self.epoch_train_loss = None\n        self.epoch_val_loss = None\n        self.epoch_val_rec_loss = None\n        self.validity_check_happened = False\n        self.cnfg = cnfg\n        self.test_sample = test_sample\n        # self.set_test_sample = False\n\n        self.df_train = df_train\n        self.df_val = df_val\n\n        self.validation_step_outs = []\n        self.train_step_outs = []\n\n    def forward(self, inpt):\n        return self.model(inpt)\n\n    def train_dataloader(self):\n        # train\n        train_dataset = MyImageDataset(\n            self.df_train, augmentations=get_train_transforms(self.cnfg)\n        )\n        train_dl = DataLoader(train_dataset, batch_size=self.cnfg.bs, shuffle=True)\n        return train_dl\n\n    def val_dataloader(self):\n        # valid\n        valid_dataset = MyImageDataset(\n            self.df_val, augmentations=get_test_transforms(self.cnfg)\n        )\n        valid_dl = DataLoader(valid_dataset, batch_size=self.cnfg.bs, shuffle=False)\n        return valid_dl\n\n    def training_step(self, x, batch_idx):\n        loss, pred, mask = self.forward(x)\n\n        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n\n        self.train_step_outs.append({\"loss\": loss})\n        return loss\n\n    def on_train_epoch_end(self):\n        outputs = [\n            k[\"loss\"].sum().item() / self.cnfg.num_gpus for k in self.train_step_outs\n        ]\n        self.epoch_train_loss = sum(outputs) / len(outputs)\n\n        self.train_step_outs = []\n        return\n\n    def validation_step(self, x, batch_idx):\n        with torch.no_grad():\n            # if (( not self.set_test_sample) and (self.validity_check_happened)):\n            #     self.test_sample = x\n            #     self.set_test_sample = True\n\n            val_loss, pred, mask = self.forward(x)\n\n            self.log(\"val_loss\", val_loss, on_epoch=True, prog_bar=True, logger=True)\n\n        self.validation_step_outs.append({\"val_loss\": val_loss})\n        return {\"val_loss\": val_loss}\n\n    def on_validation_epoch_end(self):\n        if self.validity_check_happened:\n            # print(validation_step_outputs)\n            val_losses = [\n                k[\"val_loss\"].sum().item() / self.cnfg.num_gpus\n                for k in self.validation_step_outs\n            ]\n\n            self.epoch_val_loss = sum(val_losses) / len(val_losses)\n\n            print(\n                f\"Epoch:{self.current_epoch} |Train Loss:{self.epoch_train_loss}|Valid Loss:{self.epoch_val_loss}\"\n            )\n            if self.epoch_val_loss &lt;= self.best_loss:\n                print(\n                    f\"{g_}Rec Loss Decreased from {self.best_loss} to {self.epoch_val_loss}{sr_}\"\n                )\n\n                self.best_loss = self.epoch_val_loss\n                torch.save(\n                    self.model.state_dict(),\n                    f\"{self.cnfg.MODELS_DIR}{self.cnfg.model_name}.bin\",\n                )\n                with torch.no_grad():\n                    generate_and_save_images_mae(\n                        self.model,\n                        self.current_epoch,\n                        self.test_sample,\n                        self.best_loss,\n                        self.cnfg,\n                    )\n        else:\n            self.validity_check_happened = True\n            print(\"Validity check done!\")\n\n        self.validation_step_outs = []\n\n    def configure_optimizers(self):\n        return optim.AdamW(\n            self.parameters(),\n            lr=self.cnfg.lr,\n            betas=self.cnfg.betas,\n            weight_decay=self.cnfg.weight_decay,\n        )\n\n\n\nsource\n\n\nget_model_checkpoint_callback\n\n get_model_checkpoint_callback (cnfg)\n\n\n\nExported source\ndef get_model_checkpoint_callback(cnfg):\n    checkpoint_callback = ModelCheckpoint(\n        monitor=\"train_loss\",\n        dirpath=cnfg.CHECKPOINTS_DIR + cnfg.model_name,\n        filename=cnfg.model_name + \"_{epoch}-{train_loss:.5f}-{val_loss:.5f}\",\n        save_top_k=2,\n        mode=\"min\",\n    )\n\n    return checkpoint_callback\n\n\n\nsource\n\n\nget_trainer_vq\n\n get_trainer_vq (cnfg, checkpoint_callback)\n\n\n\nExported source\ndef get_trainer_vq(cnfg, checkpoint_callback):\n    trainer = Trainer(\n        max_epochs=cnfg.epochs,\n        fast_dev_run=False,\n        callbacks=[checkpoint_callback],\n        accelerator=\"gpu\",\n        devices=1,\n        default_root_dir=cnfg.TRAINING_LOGS_DIR,\n    )\n\n    return trainer\n\n\n\nsource\n\n\nget_trainer_mae\n\n get_trainer_mae (cnfg, checkpoint_callback)\n\n\n\nExported source\ndef get_trainer_mae(cnfg, checkpoint_callback):\n    trainer = Trainer(\n        max_epochs=cnfg.epochs,\n        fast_dev_run=False,\n        callbacks=[checkpoint_callback],\n        # strategy=\"dp\",\n        accelerator=\"gpu\",\n        # gpus=cnfg.gpus,\n        devices=1,\n        default_root_dir=cnfg.TRAINING_LOGS_DIR,\n        # num_sanity_val_steps=0\n    )\n\n    return trainer",
    "crumbs": [
      "Define Lightning based trainers"
    ]
  },
  {
    "objectID": "inference_pipeline.html",
    "href": "inference_pipeline.html",
    "title": "Define Pipeline to generate embeddings",
    "section": "",
    "text": "Exported source\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom advanced_autoencoders.config import ConfigMaeLarge, ConfigVQVAE\nfrom advanced_autoencoders.dataset import MyImageDataset\nfrom advanced_autoencoders.models import (\n    get_embeddings_mae,\n    get_embeddings_vae,\n    get_mae_model,\n    get_vae_model,\n)\nfrom advanced_autoencoders.utils import (\n    get_test_transforms,\n    make_images_dataframe,\n    seed_everything,\n)\n\n\n\nsource\n\nget_test_data_loader\n\n get_test_data_loader (df, cnfg)\n\n\n\nExported source\ndef get_test_data_loader(df, cnfg):\n    dataset = MyImageDataset(df, augmentations=get_test_transforms(cnfg))\n    dl = DataLoader(dataset, batch_size=32, shuffle=False)\n\n    return dl\n\n\n\nsource\n\n\nload_model\n\n load_model (cnfg, model)\n\n\n\nExported source\ndef load_model(cnfg, model):\n    model.load_state_dict(torch.load(f\"{cnfg.MODELS_DIR}{cnfg.model_name}.bin\"))\n    model.cuda()\n    model.eval()\n\n    return model\n\n\n\nsource\n\n\ngenerate_embeddings_df\n\n generate_embeddings_df (cnfg, model, dl, embd_name, get_embd_fnc)\n\n\n\nExported source\ndef generate_embeddings_df(cnfg, model, dl, embd_name, get_embd_fnc):\n    all_embeddings = []\n    with torch.no_grad():\n        for i, samples in enumerate(tqdm(dl)):\n            embeddings = get_embd_fnc(model, samples.cuda())\n            # embeddings = torch.flatten(encoded, start_dim=1).cpu().numpy()\n            all_embeddings.extend(embeddings)\n\n    final_df = pd.DataFrame(\n        data=np.array(all_embeddings),\n        columns=cnfg.EMBEDDING_COL_NAMES,\n    )\n    final_df[embd_name] = new_df[embd_name].values\n\n    return final_df\n\n\n\nsource\n\n\nsave_data\n\n save_data (df, cnfg)\n\n\n\nExported source\ndef save_data(df, cnfg):\n    df.to_csv(cnfg.EMBEDDING_FILE_PATH, index=False)\n\n\n\nsource\n\n\ngenerate_embedding_mae_pipeline\n\n generate_embedding_mae_pipeline ()\n\n\n\nExported source\ndef generate_embedding_mae_pipeline():\n    CONFIG = ConfigMaeLarge()\n    seed_everything(seed=CONFIG.seed)\n    df_all = make_images_dataframe(CONFIG)\n    print(df_all.shape, df_all.image_name.unique().size)\n    dl = get_test_data_loader(df_all, CONFIG)\n    model = get_mae_model(cnfg)\n    model = load_model(CONFIG, model)\n\n    final_df = generate_embeddings_df(CONFIG, model, dl, \"mae_emb\", get_embeddings_mae)\n\n    save_data(final_df, CONFIG)\n    return\n\n\n\nsource\n\n\ngenerate_embedding_vqvae_pipeline\n\n generate_embedding_vqvae_pipeline ()\n\n\n\nExported source\ndef generate_embedding_vqvae_pipeline():\n    CONFIG = ConfigVQVAE()\n    seed_everything(seed=CONFIG.seed)\n    df_all = make_images_dataframe(CONFIG)\n    print(df_all.shape, df_all.image_name.unique().size)\n    dl = get_test_data_loader(df_all, CONFIG)\n    model = get_vae_model(cnfg)\n    model = load_model(CONFIG, model)\n\n    final_df = generate_embeddings_df(CONFIG, model, dl, \"vae_emb\", get_embeddings_vae)\n\n    save_data(final_df, CONFIG)\n    return",
    "crumbs": [
      "Define Pipeline to generate embeddings"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Define models",
    "section": "",
    "text": "Exported source\nfrom functools import partial\nfrom math import log2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\nfrom timm.models.vision_transformer import Block, PatchEmbed\n\nfrom advanced_autoencoders.utils import get_2d_sincos_pos_embed\n\n\n\nsource\n\nResBlock\n\n ResBlock (chan)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass ResBlock(nn.Module):\n    def __init__(self, chan):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(chan, chan, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(chan, chan, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(chan, chan, 1),\n        )\n\n    def forward(self, x):\n        return self.net(x) + x\n\n\n\nsource\n\n\nVectorQuantize\n\n VectorQuantize (dim, n_embed, commitment=1.0)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass VectorQuantize(nn.Module):\n    def __init__(self, dim, n_embed, commitment=1.0):\n        super().__init__()\n\n        self.dim = dim\n        self.n_embed = n_embed\n        self.commitment = commitment\n\n        self.embed = nn.Embedding(self.n_embed, self.dim)\n        self.embed.weight.data.uniform_(-1 / self.n_embed, 1 / self.n_embed)\n\n    def forward(self, input):\n        dtype = input.dtype  # noqa\n        input_shape = input.shape\n        flatten = input.reshape(-1, self.dim)\n\n        distances = torch.cdist(flatten, self.embed.weight)\n        encoding_index = torch.argmin(distances, dim=1)\n\n        quantized = torch.index_select(self.embed.weight, 0, encoding_index).view(\n            input_shape\n        )\n\n        return quantized\n\n\n\nsource\n\n\nMyVQVAE\n\n MyVQVAE (image_size=256, num_tokens=8192, codebook_dim=8, num_layers=4,\n          num_resnet_blocks=2, hidden_dim=64, channels=3,\n          smooth_l1_loss=False, commitment_weight=0.25,\n          use_vq_commit_loss=True)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage_size\nint\n256\n\n\n\nnum_tokens\nint\n8192\n\n\n\ncodebook_dim\nint\n8\n\n\n\nnum_layers\nint\n4\n\n\n\nnum_resnet_blocks\nint\n2\n\n\n\nhidden_dim\nint\n64\n\n\n\nchannels\nint\n3\n\n\n\nsmooth_l1_loss\nbool\nFalse\n\n\n\ncommitment_weight\nfloat\n0.25\nvq_decay = 0.8,\n\n\nuse_vq_commit_loss\nbool\nTrue\n\n\n\n\n\n\nExported source\nclass MyVQVAE(nn.Module):\n    def __init__(\n        self,\n        image_size=256,\n        num_tokens=8192,\n        codebook_dim=8,\n        num_layers=4,\n        num_resnet_blocks=2,\n        hidden_dim=64,\n        channels=3,\n        smooth_l1_loss=False,\n        # vq_decay = 0.8,\n        commitment_weight=0.25,\n        use_vq_commit_loss=True,\n    ):\n        super().__init__()\n        assert log2(image_size).is_integer(), \"image size must be a power of 2\"\n        assert num_layers &gt;= 1, \"number of layers must be greater than or equal to 1\"\n        has_resblocks = num_resnet_blocks &gt; 0\n\n        self.image_size = image_size\n        self.num_tokens = num_tokens\n        self.num_layers = num_layers\n        self.commitment_weight = commitment_weight\n        self.use_vq_commit_loss = use_vq_commit_loss\n\n        self.vq = VectorQuantize(\n            dim=codebook_dim,\n            n_embed=num_tokens,\n            # decay = vq_decay,\n            commitment=commitment_weight,\n        )\n\n        hdim = hidden_dim  # noqa\n\n        enc_chans = [hidden_dim] * num_layers\n        dec_chans = list(reversed(enc_chans))\n\n        enc_chans = [channels, *enc_chans]\n\n        dec_init_chan = codebook_dim if not has_resblocks else dec_chans[0]\n        dec_chans = [dec_init_chan, *dec_chans]\n\n        enc_chans_io, dec_chans_io = map(\n            lambda t: list(zip(t[:-1], t[1:])), (enc_chans, dec_chans)\n        )\n\n        enc_layers = []\n        dec_layers = []\n\n        for (enc_in, enc_out), (dec_in, dec_out) in zip(enc_chans_io, dec_chans_io):\n            enc_layers.append(\n                nn.Sequential(\n                    nn.Conv2d(enc_in, enc_out, 4, stride=2, padding=1), nn.ReLU()\n                )\n            )\n            dec_layers.append(\n                nn.Sequential(\n                    nn.ConvTranspose2d(dec_in, dec_out, 4, stride=2, padding=1),\n                    nn.ReLU(),\n                )\n            )\n\n        for _ in range(num_resnet_blocks):\n            dec_layers.insert(0, ResBlock(dec_chans[1]))\n            enc_layers.append(ResBlock(enc_chans[-1]))\n\n        if num_resnet_blocks &gt; 0:\n            dec_layers.insert(0, nn.Conv2d(codebook_dim, dec_chans[1], 1))\n\n        enc_layers.append(nn.Conv2d(enc_chans[-1], codebook_dim, 1))\n        dec_layers.append(nn.Conv2d(dec_chans[-1], channels, 1))\n\n        self.encoder = nn.Sequential(*enc_layers)\n        self.decoder = nn.Sequential(*dec_layers)\n\n        self.loss_fn = F.smooth_l1_loss if smooth_l1_loss else F.mse_loss\n\n    def get_vq_commitment_loss(self, quantized, inputs):\n        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n        c_loss = q_latent_loss + self.commitment_weight * e_latent_loss\n\n        quantized = inputs + (quantized - inputs).detach()\n\n        return quantized, c_loss\n\n    def forward(\n        self,\n        img,\n        return_loss=False,\n        return_recons=False,\n        return_encoded=False,\n    ):\n        shape, device = img.shape, img.device  # noqa\n\n        encoded = self.encoder(img)\n\n        if return_encoded:\n            return encoded\n\n        h, w = encoded.shape[-2:]\n\n        encoded = rearrange(encoded, \"b c h w -&gt; b (h w) c\")\n        # quantized, _ = self.vq(encoded)\n        quantized = self.vq(encoded)\n\n        if self.use_vq_commit_loss:\n            quantized, c_loss = self.get_vq_commitment_loss(quantized, encoded)\n\n        quantized = rearrange(quantized, \"b (h w) c -&gt; b c h w\", h=h, w=w)\n        out = self.decoder(quantized)\n\n        if not return_loss:\n            return out\n\n        # reconstruction loss and VQ commitment loss\n\n        recon_loss = self.loss_fn(img, out)\n\n        if self.use_vq_commit_loss:\n            loss = recon_loss + c_loss\n        else:\n            loss = recon_loss\n\n        if not return_recons:\n            return loss, recon_loss\n\n        return loss, recon_loss, out\n\n\n\nsource\n\n\nMaskedAutoencoderViT\n\n MaskedAutoencoderViT (img_size=256, patch_size=8, mask_ratio=0.75,\n                       in_chans=3, embed_dim=256, depth=24, num_heads=16,\n                       decoder_embed_dim=128, decoder_depth=8,\n                       decoder_num_heads=16, mlp_ratio=4.0,\n                       norm_layer=&lt;class\n                       'torch.nn.modules.normalization.LayerNorm'&gt;,\n                       norm_pix_loss=False)\n\nMasked Autoencoder with VisionTransformer backbone\n\n\nExported source\nclass MaskedAutoencoderViT(nn.Module):\n    \"\"\"Masked Autoencoder with VisionTransformer backbone\"\"\"\n\n    def __init__(\n        self,\n        img_size=256,\n        patch_size=8,\n        mask_ratio=0.75,\n        in_chans=3,\n        embed_dim=256,\n        depth=24,\n        num_heads=16,\n        decoder_embed_dim=128,\n        decoder_depth=8,\n        decoder_num_heads=16,\n        mlp_ratio=4.0,\n        norm_layer=nn.LayerNorm,\n        norm_pix_loss=False,\n    ):\n        super().__init__()\n\n        self.mask_ratio = mask_ratio\n\n        # --------------------------------------------------------------------------\n        # MAE encoder specifics\n        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False\n        )  # fixed sin-cos embedding\n\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    embed_dim,\n                    num_heads,\n                    mlp_ratio,\n                    qkv_bias=True,\n                    # qk_scale=None,\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = norm_layer(embed_dim)\n        # --------------------------------------------------------------------------\n\n        # --------------------------------------------------------------------------\n        # MAE decoder specifics\n        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n\n        self.decoder_pos_embed = nn.Parameter(\n            torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False\n        )  # fixed sin-cos embedding\n\n        self.decoder_blocks = nn.ModuleList(\n            [\n                Block(\n                    decoder_embed_dim,\n                    decoder_num_heads,\n                    mlp_ratio,\n                    qkv_bias=True,\n                    # qk_scale=None,\n                    norm_layer=norm_layer,\n                )\n                for i in range(decoder_depth)\n            ]\n        )\n\n        self.decoder_norm = norm_layer(decoder_embed_dim)\n        self.decoder_pred = nn.Linear(\n            decoder_embed_dim, patch_size**2 * in_chans, bias=True\n        )  # decoder to patch\n        # --------------------------------------------------------------------------\n\n        self.norm_pix_loss = norm_pix_loss\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        # initialization\n        # initialize (and freeze) pos_embed by sin-cos embedding\n        pos_embed = get_2d_sincos_pos_embed(\n            self.pos_embed.shape[-1],\n            int(self.patch_embed.num_patches**0.5),\n            cls_token=True,\n        )\n        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n\n        decoder_pos_embed = get_2d_sincos_pos_embed(\n            self.decoder_pos_embed.shape[-1],\n            int(self.patch_embed.num_patches**0.5),\n            cls_token=True,\n        )\n        self.decoder_pos_embed.data.copy_(\n            torch.from_numpy(decoder_pos_embed).float().unsqueeze(0)\n        )\n\n        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n        w = self.patch_embed.proj.weight.data\n        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n\n        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n        torch.nn.init.normal_(self.cls_token, std=0.02)\n        torch.nn.init.normal_(self.mask_token, std=0.02)\n\n        # initialize nn.Linear and nn.LayerNorm\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            # we use xavier_uniform following official JAX ViT:\n            torch.nn.init.xavier_uniform_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def patchify(self, imgs):\n        \"\"\"\n        imgs: (N, 3, H, W)\n        x: (N, L, patch_size**2 *3)\n        \"\"\"\n        p = self.patch_embed.patch_size[0]\n        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n\n        h = w = imgs.shape[2] // p\n        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n        x = torch.einsum(\"nchpwq-&gt;nhwpqc\", x)\n        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n        return x\n\n    def unpatchify(self, x):\n        \"\"\"\n        x: (N, L, patch_size**2 *3)\n        imgs: (N, 3, H, W)\n        \"\"\"\n        p = self.patch_embed.patch_size[0]\n        h = w = int(x.shape[1] ** 0.5)\n        assert h * w == x.shape[1]\n\n        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n        x = torch.einsum(\"nhwpqc-&gt;nchpwq\", x)\n        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n        return imgs\n\n    def random_masking(self, x):\n        \"\"\"\n        Perform per-sample random masking by per-sample shuffling.\n        Per-sample shuffling is done by argsort random noise.\n        x: [N, L, D], sequence\n        \"\"\"\n        N, L, D = x.shape  # batch, length, dim\n        len_keep = int(L * (1 - self.mask_ratio))\n\n        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n\n        # sort noise for each sample\n        ids_shuffle = torch.argsort(\n            noise, dim=1\n        )  # ascend: small is keep, large is remove\n        ids_restore = torch.argsort(ids_shuffle, dim=1)\n\n        # keep the first subset\n        ids_keep = ids_shuffle[:, :len_keep]\n        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n\n        # generate the binary mask: 0 is keep, 1 is remove\n        mask = torch.ones([N, L], device=x.device)\n        mask[:, :len_keep] = 0\n        # unshuffle to get the binary mask\n        mask = torch.gather(mask, dim=1, index=ids_restore)\n\n        return x_masked, mask, ids_restore\n\n    def forward_encoder(self, x):\n        # embed patches\n        x = self.patch_embed(x)\n\n        # add pos embed w/o cls token\n        x = x + self.pos_embed[:, 1:, :]\n\n        # masking: length -&gt; length * mask_ratio\n        x, mask, ids_restore = self.random_masking(x)\n\n        # append cls token\n        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        # apply Transformer blocks\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n\n        return x, mask, ids_restore\n\n    def forward_encoder_features(self, x):\n        # embed patches\n        x = self.patch_embed(x)\n\n        # add pos embed w/o cls token\n        x = x + self.pos_embed[:, 1:, :]\n\n        # append cls token\n        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        # apply Transformer blocks\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n\n        return x\n\n    def forward_decoder(self, x, ids_restore):\n        # embed tokens\n        x = self.decoder_embed(x)\n\n        # append mask tokens to sequence\n        mask_tokens = self.mask_token.repeat(\n            x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1\n        )\n        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n        x_ = torch.gather(\n            x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2])\n        )  # unshuffle\n        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n\n        # add pos embed\n        x = x + self.decoder_pos_embed\n\n        # apply Transformer blocks\n        for blk in self.decoder_blocks:\n            x = blk(x)\n        x = self.decoder_norm(x)\n\n        # predictor projection\n        x = self.decoder_pred(x)\n\n        # remove cls token\n        x = x[:, 1:, :]\n\n        return x\n\n    def forward_decoder2(self, x):\n        # embed tokens\n        x = self.decoder_embed(x)\n\n        # add pos embed\n        x = x + self.decoder_pos_embed\n\n        # apply Transformer blocks\n        for blk in self.decoder_blocks:\n            x = blk(x)\n        x = self.decoder_norm(x)\n\n        # predictor projection\n        x = self.decoder_pred(x)\n\n        # remove cls token\n        x = x[:, 1:, :]\n\n        return x\n\n    def forward_loss(self, imgs, pred, mask):\n        \"\"\"\n        imgs: [N, 3, H, W]\n        pred: [N, L, p*p*3]\n        mask: [N, L], 0 is keep, 1 is remove,\n        \"\"\"\n        target = self.patchify(imgs)\n        if self.norm_pix_loss:\n            mean = target.mean(dim=-1, keepdim=True)\n            var = target.var(dim=-1, keepdim=True)\n            target = (target - mean) / (var + 1.0e-6) ** 0.5\n\n        loss = (pred - target) ** 2\n        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n\n        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n        return loss\n\n    def forward(self, imgs):\n        latent, mask, ids_restore = self.forward_encoder(imgs)\n        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n        loss = self.forward_loss(imgs, pred, mask)\n        return loss, pred, mask\n\n    def forward2(self, imgs):\n        latent = self.forward_encoder_features(imgs)\n        pred = self.forward_decoder2(latent)  # [N, L, p*p*3]\n        return pred\n\n\n\nsource\n\n\nget_mae_model\n\n get_mae_model (cnfg)\n\n\n\nExported source\ndef get_mae_model(cnfg):\n    model = MaskedAutoencoderViT(\n        patch_size=cnfg.patch_size,\n        embed_dim=cnfg.encoder_embedding_dim,\n        depth=cnfg.depth_enc,\n        num_heads=cnfg.num_heads_enc,\n        decoder_embed_dim=cnfg.decoder_embedding_dim,\n        decoder_depth=cnfg.decoder_depth,\n        decoder_num_heads=cnfg.decoder_num_heads,\n        mlp_ratio=cnfg.mlp_ratio,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        img_size=cnfg.img_size,\n        in_chans=cnfg.in_chans,\n        mask_ratio=cnfg.mask_ratio,\n    )\n\n    return model\n\n\n\nsource\n\n\nget_embeddings_mae\n\n get_embeddings_mae (model, samples)\n\n\n\nExported source\ndef get_embeddings_mae(model, samples):\n    out = model.forward_encoder_features(samples)\n    cls_token = torch.select(out, 1, 0)\n\n    return cls_token\n\n\n\nsource\n\n\nget_vae_model\n\n get_vae_model (cnfg)\n\n\n\nExported source\ndef get_vae_model(cnfg):\n    vae = MyVQVAE(\n        image_size=cnfg.img_size,\n        num_layers=cnfg.num_layers,  # number of downsamples - ex. 256 / (2 ** 3) = (32 x 32 feature map)\n        num_tokens=cnfg.num_tokens,  # number of visual tokens. in the paper, they used 8192, but could be smaller for downsized projects\n        codebook_dim=cnfg.codebook_dim,  # codebook dimension\n        hidden_dim=cnfg.hidden_dim,  # hidden dimension\n        num_resnet_blocks=cnfg.num_resnet_blocks,  # number of resnet blocks\n        use_vq_commit_loss=cnfg.use_vq_commit_loss,\n    )\n\n    return vae\n\n\n\nsource\n\n\nget_embeddings_vae\n\n get_embeddings_vae (dvae_model, img)\n\n\n\nExported source\ndef get_embeddings_vae(dvae_model, img):\n    encoded = dvae_model.encoder(img)\n\n    h, w = encoded.shape[-2:]\n\n    encoded = rearrange(encoded, \"b c h w -&gt; b (h w) c\")\n\n    quantized = dvae_model.vq(encoded)\n\n    if dvae_model.use_vq_commit_loss:\n        quantized, c_loss = dvae_model.get_vq_commitment_loss(quantized, encoded)\n\n    quantized = rearrange(quantized, \"b (h w) c -&gt; b c h w\", h=h, w=w)\n\n    return quantized",
    "crumbs": [
      "Define models"
    ]
  },
  {
    "objectID": "dataset.html",
    "href": "dataset.html",
    "title": "Define Datasets for dataloaders",
    "section": "",
    "text": "Exported source\nimport cv2\nimport PIL\nfrom torch.utils.data import Dataset\n\n\n\nsource\n\nMyImageDataset\n\n MyImageDataset (df, augmentations)\n\n*An abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.*\n\n\nExported source\nclass MyImageDataset(Dataset):\n    def __init__(self, df, augmentations):\n        self.df = df\n        self.augmentations = augmentations\n\n    def __getitem__(self, idx):\n        path = self.df.iloc[idx][\"file_image\"]\n        image = cv2.imread(path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.augmentations:\n            image = self.augmentations(PIL.Image.fromarray(image))\n            # image = augmented[\"image\"]\n\n        return image\n\n    def __len__(self):\n        return len(self.df)",
    "crumbs": [
      "Define Datasets for dataloaders"
    ]
  },
  {
    "objectID": "eda_data.html",
    "href": "eda_data.html",
    "title": "EDA on image dataset",
    "section": "",
    "text": "import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torchvision\nimport torchvision.transforms.functional as F\nfrom torchvision.utils import make_grid\n\nfrom advanced_autoencoders.config import ConfigMaeLarge\nfrom advanced_autoencoders.inference import get_test_data_loader\nfrom advanced_autoencoders.utils import make_images_dataframe\n\n\ncnfg = ConfigMaeLarge()\n\n\ndf_all = make_images_dataframe(cnfg)\n\n\ndf_all.image_sufix.value_counts()\n\nimage_sufix\n.jpg    64467\nName: count, dtype: int64\n\n\n\n# path = df_all.sample(1).iloc[0][\"file_image\"]\npath = df_all.iloc[0][\"file_image\"]\nimage = cv2.imread(path, cv2.IMREAD_COLOR)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n\nplt.imshow(image)\n\n\n\n\n\n\n\n\n\ntest_dl = get_test_data_loader(df_all, cnfg)\n\n\ntest_dataloader_iterator = iter(test_dl)\nsample_batch = next(test_dataloader_iterator)\n\n\nsample_batch.shape\n\ntorch.Size([32, 3, 256, 256])\n\n\n\ntmp = [i for i in sample_batch[:8]]\ngrid = make_grid(tmp)\n\n\n# display result\nimg = torchvision.transforms.ToPILImage()(grid)\n\n\nimg",
    "crumbs": [
      "EDA on image dataset"
    ]
  },
  {
    "objectID": "training_pipeline.html",
    "href": "training_pipeline.html",
    "title": "Define: Training Pipeline",
    "section": "",
    "text": "Exported source\nimport warnings\n\nimport pandas as pd\nfrom colorama import Fore, Style\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\n\nfrom advanced_autoencoders.config import ConfigMaeLarge, ConfigVQVAE\nfrom advanced_autoencoders.dataset import MyImageDataset\nfrom advanced_autoencoders.trainers import (\n    PlModelMAE,\n    PlModelVQVAE,\n    get_model_checkpoint_callback,\n    get_trainer_mae,\n    get_trainer_vq,\n)\nfrom advanced_autoencoders.utils import (\n    get_train_transforms,\n    make_images_dataframe,\n    seed_everything,\n)\n\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL\nwarnings.filterwarnings(\"ignore\")\n\n\n\nsource\n\nmake_train_test_split\n\n make_train_test_split (df:pandas.core.frame.DataFrame, cnfg)\n\n\n\nExported source\ndef make_train_test_split(df: pd.DataFrame, cnfg):\n    df_train, df_val = train_test_split(df, test_size=cnfg.size_val, random_state=42)\n\n    return df_train, df_val\n\n\n\nsource\n\n\nget_reconstruction_sample\n\n get_reconstruction_sample (cnfg, df)\n\n\n\nExported source\ndef get_reconstruction_sample(cnfg, df):\n    val_reconstruction_dataset = MyImageDataset(\n        df, augmentations=get_train_transforms(cnfg)\n    )\n    val_dl_rec = DataLoader(val_reconstruction_dataset, batch_size=4, shuffle=False)\n    for test_sample in val_dl_rec:\n        break\n    test_sample = test_sample.cuda(f\"cuda:{cnfg.gpus[0]}\")\n\n    return test_sample\n\n\n\nsource\n\n\ntrain_pipeline_mae\n\n train_pipeline_mae ()\n\n\n\nExported source\ndef train_pipeline_mae():\n    CONFIG = ConfigMaeLarge()\n    seed_everything(seed=CONFIG.seed)\n    df_all = make_images_dataframe(CONFIG)\n    print(f\"Making Image DF from DIR DONE! TOTAL IMAGES:{df_all.shape[0]}\")\n    df_train, df_val = make_train_test_split(df_all, CONFIG)\n    print(\n        f\"TRAIN VAL SPLIT DONE! TOTAL IMAGES TRAIN:{df_train.shape[0]}, VAL:{df_val.shape[0]}\"\n    )\n    test_sample = get_reconstruction_sample(CONFIG, df_val)\n    checkpoint_callback = get_model_checkpoint_callback(CONFIG)\n    trainer = get_trainer_mae(CONFIG, checkpoint_callback)\n    pl_model = PlModelMAE(CONFIG, df_train, df_val, test_sample)\n\n    trainer.fit(pl_model)\n\n\n\nsource\n\n\ntrain_pipeline_vqvae\n\n train_pipeline_vqvae ()\n\n\n\nExported source\ndef train_pipeline_vqvae():\n    CONFIG = ConfigVQVAE()\n    seed_everything(seed=CONFIG.seed)\n    df_all = make_images_dataframe(CONFIG)\n    print(f\"Making Image DF from DIR DONE! TOTAL IMAGES:{df_all.shape[0]}\")\n    df_train, df_val = make_train_test_split(df_all, CONFIG)\n    print(\n        f\"TRAIN VAL SPLIT DONE! TOTAL IMAGES TRAIN:{df_train.shape[0]}, VAL:{df_val.shape[0]}\"\n    )\n    test_sample = get_reconstruction_sample(CONFIG, df_val)\n    checkpoint_callback = get_model_checkpoint_callback(CONFIG)\n    trainer = get_trainer_vq(CONFIG, checkpoint_callback)\n    pl_model = PlModelVQVAE(CONFIG, df_train, df_val, test_sample)\n\n    trainer.fit(pl_model)",
    "crumbs": [
      "Define: Training Pipeline"
    ]
  },
  {
    "objectID": "config.html",
    "href": "config.html",
    "title": "Configs for ViTMAE and VQ-MAE",
    "section": "",
    "text": "source\n\nConfigMaeLarge\n\n ConfigMaeLarge ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass ConfigMaeLarge:\n    seed = 1000\n\n    model_name = \"vitmae_fb_large_v5\"\n    prev_model_name = None\n\n    img_size = 256\n    in_chans = 3\n    lr = 1e-4\n    bs = 8\n    epochs = 2\n    weight_decay = 0.05\n    betas = (0.9, 0.95)\n\n    gpus = [0]\n    num_gpus = len(gpus)\n\n    embd_dim = 256\n\n    # Masked AutoEncoder Params\n    encoder_embedding_dim = embd_dim\n    depth_enc = 24\n    num_heads_enc = 16\n\n    decoder_embedding_dim = 128\n    decoder_depth = 8\n    decoder_num_heads = 16\n\n    mlp_ratio = 4.0\n\n    patch_size = 8\n    mask_ratio = 0.75\n\n    DATA_DIR = \"../data/\"\n    IMG_DIR = DATA_DIR + \"cars_dataset/\"\n    MODELS_DIR = \"../models/\"\n\n    TRAINING_LOGS_DIR = f\"../training_logs/{model_name}/\"\n    CHECKPOINTS_DIR = TRAINING_LOGS_DIR + \"checkpoints/\"\n\n    TRAINING_RECON_IMG_DIR = f\"{TRAINING_LOGS_DIR}{model_name}/\"\n\n    embedding_name = \"final2\"\n    EMBEDDINGS_DIR = DATA_DIR + \"embeddings/\"\n    EMBEDDING_COL_NAMES = [f\"dim_{k}\" for k in range(encoder_embedding_dim)]\n    EMBEDDING_FILE_PATH = (\n        f\"{EMBEDDINGS_DIR}{model_name}_{encoder_embedding_dim}_{embedding_name}.csv\"\n    )\n\n    size_val = 0.2  # Size of validation set\n\n\n\nsource\n\n\nConfigVQVAE\n\n ConfigVQVAE ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass ConfigVQVAE:\n    seed = 1000\n\n    DATA_DIR = \"../data/\"\n    IMG_DIR = DATA_DIR + \"cars_dataset/\"\n    MODELS_DIR = \"../models/\"\n\n    img_size = 256\n\n    gpus = [0]\n    num_gpus = len(gpus)\n\n    # Training Params\n    lr = 1e-3\n    bs = 64\n    epochs = 100\n    size_val = 0.2  # Size of validation set\n\n    embd_dim = 2048\n\n    if embd_dim == 512:\n        training_name = \"16_16_2_v1\"\n        num_layers = (\n            4  # number of downsamples - ex. 256 / (2 ** 3) = (32 x 32 feature map)\n        )\n    else:\n        training_name = \"32_32_2_v2\"\n        num_layers = (\n            3  # number of downsamples - ex. 256 / (2 ** 3) = (32 x 32 feature map)\n        )\n\n    # Model Config\n    model_name = f\"dalle_vq_vae_{training_name}\"\n\n    num_tokens = 8192  # number of visual tokens. in the paper, they used 8192, but could be smaller for downsized projects\n    codebook_dim = 2  # codebook dimension\n    hidden_dim = 128  # hidden dimension\n    num_resnet_blocks = 2  # number of resnet blocks\n    use_vq_commit_loss = True\n\n    # Logging\n    TRAINING_LOGS_DIR = f\"../training_logs/{model_name}/\"\n    TRAINING_RECON_IMG_DIR = f\"{TRAINING_LOGS_DIR}{model_name}/\"\n    CHECKPOINTS_DIR = f\"{TRAINING_LOGS_DIR}checkpoints/\"\n\n    # Embeddings\n    EMBEDDINGS_DIR = DATA_DIR + \"embeddings/\"\n    EMBEDDING_FILE_PATH = f\"{EMBEDDINGS_DIR}{model_name}_{embd_dim}.csv\"\n    EMBEDDING_COL_NAMES = [f\"dim_{k}\" for k in range(embd_dim)]",
    "crumbs": [
      "Configs for ViTMAE and VQ-MAE"
    ]
  }
]