{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: config.html\n",
    "title: Configs for ViTMAE and VQ-MAE\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/rohitMalhotra07/advanced_autoencoders/blob/main/advanced_autoencoders/config.py#L7){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ConfigMaeLarge\n",
       "\n",
       ">      ConfigMaeLarge ()\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/rohitMalhotra07/advanced_autoencoders/blob/main/advanced_autoencoders/config.py#L7){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ConfigMaeLarge\n",
       "\n",
       ">      ConfigMaeLarge ()\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ConfigMaeLarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e3558-88b0-4e3e-ab49-b43170f51537",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "#| code-summary: \"Exported source\"\n",
    "class ConfigMaeLarge:\n",
    "    seed = 1000\n",
    "\n",
    "    model_name = \"vitmae_fb_large_v5\"\n",
    "    prev_model_name = None\n",
    "\n",
    "    img_size = 256\n",
    "    in_chans = 3\n",
    "    lr = 1e-4\n",
    "    bs = 8\n",
    "    epochs = 2\n",
    "    weight_decay = 0.05\n",
    "    betas = (0.9, 0.95)\n",
    "\n",
    "    gpus = [0]\n",
    "    num_gpus = len(gpus)\n",
    "\n",
    "    embd_dim = 256\n",
    "\n",
    "    # Masked AutoEncoder Params\n",
    "    encoder_embedding_dim = embd_dim\n",
    "    depth_enc = 24\n",
    "    num_heads_enc = 16\n",
    "\n",
    "    decoder_embedding_dim = 128\n",
    "    decoder_depth = 8\n",
    "    decoder_num_heads = 16\n",
    "\n",
    "    mlp_ratio = 4.0\n",
    "\n",
    "    patch_size = 8\n",
    "    mask_ratio = 0.75\n",
    "\n",
    "    DATA_DIR = \"../data/\"\n",
    "    IMG_DIR = DATA_DIR + \"cars_dataset/\"\n",
    "    MODELS_DIR = \"../models/\"\n",
    "\n",
    "    TRAINING_LOGS_DIR = f\"../training_logs/{model_name}/\"\n",
    "    CHECKPOINTS_DIR = TRAINING_LOGS_DIR + \"checkpoints/\"\n",
    "\n",
    "    TRAINING_RECON_IMG_DIR = f\"{TRAINING_LOGS_DIR}{model_name}/\"\n",
    "\n",
    "    embedding_name = \"final2\"\n",
    "    EMBEDDINGS_DIR = DATA_DIR + \"embeddings/\"\n",
    "    EMBEDDING_COL_NAMES = [f\"dim_{k}\" for k in range(encoder_embedding_dim)]\n",
    "    EMBEDDING_FILE_PATH = (\n",
    "        f\"{EMBEDDINGS_DIR}{model_name}_{encoder_embedding_dim}_{embedding_name}.csv\"\n",
    "    )\n",
    "\n",
    "    size_val = 0.2  # Size of validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece276c9-0749-4c83-b3e4-99a373397116",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7b759-94c7-48db-b1a6-97642eb096a8",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/rohitMalhotra07/advanced_autoencoders/blob/main/advanced_autoencoders/config.py#L59){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ConfigVQVAE\n",
       "\n",
       ">      ConfigVQVAE ()\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/rohitMalhotra07/advanced_autoencoders/blob/main/advanced_autoencoders/config.py#L59){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ConfigVQVAE\n",
       "\n",
       ">      ConfigVQVAE ()\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ConfigVQVAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81398c0-0770-4c63-a648-3ae83d712909",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "#| code-summary: \"Exported source\"\n",
    "class ConfigVQVAE:\n",
    "    seed = 1000\n",
    "\n",
    "    DATA_DIR = \"../data/\"\n",
    "    IMG_DIR = DATA_DIR + \"cars_dataset/\"\n",
    "    MODELS_DIR = \"../models/\"\n",
    "\n",
    "    img_size = 256\n",
    "\n",
    "    gpus = [0]\n",
    "    num_gpus = len(gpus)\n",
    "\n",
    "    # Training Params\n",
    "    lr = 1e-3\n",
    "    bs = 64\n",
    "    epochs = 100\n",
    "    size_val = 0.2  # Size of validation set\n",
    "\n",
    "    embd_dim = 2048\n",
    "\n",
    "    if embd_dim == 512:\n",
    "        training_name = \"16_16_2_v1\"\n",
    "        num_layers = (\n",
    "            4  # number of downsamples - ex. 256 / (2 ** 3) = (32 x 32 feature map)\n",
    "        )\n",
    "    else:\n",
    "        training_name = \"32_32_2_v2\"\n",
    "        num_layers = (\n",
    "            3  # number of downsamples - ex. 256 / (2 ** 3) = (32 x 32 feature map)\n",
    "        )\n",
    "\n",
    "    # Model Config\n",
    "    model_name = f\"dalle_vq_vae_{training_name}\"\n",
    "\n",
    "    num_tokens = 8192  # number of visual tokens. in the paper, they used 8192, but could be smaller for downsized projects\n",
    "    codebook_dim = 2  # codebook dimension\n",
    "    hidden_dim = 128  # hidden dimension\n",
    "    num_resnet_blocks = 2  # number of resnet blocks\n",
    "    use_vq_commit_loss = True\n",
    "\n",
    "    # Logging\n",
    "    TRAINING_LOGS_DIR = f\"../training_logs/{model_name}/\"\n",
    "    TRAINING_RECON_IMG_DIR = f\"{TRAINING_LOGS_DIR}{model_name}/\"\n",
    "    CHECKPOINTS_DIR = f\"{TRAINING_LOGS_DIR}checkpoints/\"\n",
    "\n",
    "    # Embeddings\n",
    "    EMBEDDINGS_DIR = DATA_DIR + \"embeddings/\"\n",
    "    EMBEDDING_FILE_PATH = f\"{EMBEDDINGS_DIR}{model_name}_{embd_dim}.csv\"\n",
    "    EMBEDDING_COL_NAMES = [f\"dim_{k}\" for k in range(embd_dim)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
