{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c78f93-9f7d-4e39-a4b5-9745e87d3b42",
   "metadata": {},
   "source": [
    "# Define Pipeline to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1463ac-da28-48fa-b9a2-078fcfd4d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7926aa1-9a4a-443d-a8a7-3d4c339f3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from advanced_autoencoders.config import ConfigMaeLarge, ConfigVQVAE\n",
    "from advanced_autoencoders.dataset import MyImageDataset\n",
    "from advanced_autoencoders.models import (\n",
    "    get_embeddings_mae,\n",
    "    get_embeddings_vae,\n",
    "    get_mae_model,\n",
    "    get_vae_model,\n",
    ")\n",
    "from advanced_autoencoders.utils import (\n",
    "    get_test_transforms,\n",
    "    make_images_dataframe,\n",
    "    seed_everything,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4db16-8f26-4b7d-8176-e2ecd724b950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ad629-8135-4e5c-9b77-f9ef81541adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0ba05-09bb-4dc2-9c8d-8c7d4506ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_test_data_loader(df, cnfg):\n",
    "    dataset = MyImageDataset(df, augmentations=get_test_transforms(cnfg))\n",
    "    dl = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599ba70-bce2-46d3-a7eb-7aee00890657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def load_model(cnfg, model):\n",
    "    model.load_state_dict(torch.load(f\"{cnfg.MODELS_DIR}{cnfg.model_name}.bin\"))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce88dec-879d-4dd8-96c6-e003c0caefc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688190e-e5e0-4a6e-9aba-6d51e27b4391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def generate_embeddings_df(cnfg, model, dl, embd_name, get_embd_fnc):\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i, samples in enumerate(tqdm(dl)):\n",
    "            embeddings = get_embd_fnc(model, samples.cuda())\n",
    "            # embeddings = torch.flatten(encoded, start_dim=1).cpu().numpy()\n",
    "            all_embeddings.extend(embeddings)\n",
    "\n",
    "    final_df = pd.DataFrame(\n",
    "        data=np.array(all_embeddings),\n",
    "        columns=cnfg.EMBEDDING_COL_NAMES,\n",
    "    )\n",
    "    final_df[embd_name] = new_df[embd_name].values\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ac975-48d5-4a75-a954-1e0043f98048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def save_data(df, cnfg):\n",
    "    df.to_csv(cnfg.EMBEDDING_FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9001fcd-96f7-457d-9cdf-072c2be74fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619c8a0-b188-4a15-a15a-77ea370a49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def generate_embedding_mae_pipeline():\n",
    "    CONFIG = ConfigMaeLarge()\n",
    "    seed_everything(seed=CONFIG.seed)\n",
    "    df_all = make_images_dataframe(CONFIG)\n",
    "    print(df_all.shape, df_all.image_name.unique().size)\n",
    "    dl = get_test_data_loader(df_all, CONFIG)\n",
    "    model = get_mae_model(cnfg)\n",
    "    model = load_model(CONFIG, model)\n",
    "\n",
    "    final_df = generate_embeddings_df(CONFIG, model, dl, \"mae_emb\", get_embeddings_mae)\n",
    "\n",
    "    save_data(final_df, CONFIG)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed643327-6ff7-4322-884e-b19c07dd3025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b76ae-8b45-4c44-ad4d-b959a9a7542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def generate_embedding_vqvae_pipeline():\n",
    "    CONFIG = ConfigVQVAE()\n",
    "    seed_everything(seed=CONFIG.seed)\n",
    "    df_all = make_images_dataframe(CONFIG)\n",
    "    print(df_all.shape, df_all.image_name.unique().size)\n",
    "    dl = get_test_data_loader(df_all, CONFIG)\n",
    "    model = get_vae_model(cnfg)\n",
    "    model = load_model(CONFIG, model)\n",
    "\n",
    "    final_df = generate_embeddings_df(CONFIG, model, dl, \"vae_emb\", get_embeddings_vae)\n",
    "\n",
    "    save_data(final_df, CONFIG)\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
