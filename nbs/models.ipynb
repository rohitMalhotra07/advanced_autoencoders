{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541a6d93-6807-4810-839b-96b23a3004c2",
   "metadata": {},
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b2a56f-f186-4fd8-be12-d9ace7294937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25331930-5f36-4d9b-85ca-0481bb5c973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "from functools import partial\n",
    "from math import log2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from timm.models.vision_transformer import Block, PatchEmbed\n",
    "\n",
    "from advanced_autoencoders.utils import get_2d_sincos_pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76318461-903c-4d75-9013-04c9940ecf98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db099d-5076-4ad5-94c3-98edf90b36cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300361c3-4d29-48c9-8516-acd206df212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, chan):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(chan, chan, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(chan, chan, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(chan, chan, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5aeec7-0160-4f5b-8955-9fc1922d756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class VectorQuantize(nn.Module):\n",
    "    def __init__(self, dim, n_embed, commitment=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_embed = n_embed\n",
    "        self.commitment = commitment\n",
    "\n",
    "        self.embed = nn.Embedding(self.n_embed, self.dim)\n",
    "        self.embed.weight.data.uniform_(-1 / self.n_embed, 1 / self.n_embed)\n",
    "\n",
    "    def forward(self, input):\n",
    "        dtype = input.dtype  # noqa\n",
    "        input_shape = input.shape\n",
    "        flatten = input.reshape(-1, self.dim)\n",
    "\n",
    "        distances = torch.cdist(flatten, self.embed.weight)\n",
    "        encoding_index = torch.argmin(distances, dim=1)\n",
    "\n",
    "        quantized = torch.index_select(self.embed.weight, 0, encoding_index).view(\n",
    "            input_shape\n",
    "        )\n",
    "\n",
    "        return quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e03e2-72eb-46c1-aa57-43580e1a481b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6c598-8549-40e9-8be1-882b4b9e3c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d922e1-aa7d-43e3-8caa-276858f4b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class MyVQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=256,\n",
    "        num_tokens=8192,\n",
    "        codebook_dim=8,\n",
    "        num_layers=4,\n",
    "        num_resnet_blocks=2,\n",
    "        hidden_dim=64,\n",
    "        channels=3,\n",
    "        smooth_l1_loss=False,\n",
    "        # vq_decay = 0.8,\n",
    "        commitment_weight=0.25,\n",
    "        use_vq_commit_loss=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert log2(image_size).is_integer(), \"image size must be a power of 2\"\n",
    "        assert num_layers >= 1, \"number of layers must be greater than or equal to 1\"\n",
    "        has_resblocks = num_resnet_blocks > 0\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.num_tokens = num_tokens\n",
    "        self.num_layers = num_layers\n",
    "        self.commitment_weight = commitment_weight\n",
    "        self.use_vq_commit_loss = use_vq_commit_loss\n",
    "\n",
    "        self.vq = VectorQuantize(\n",
    "            dim=codebook_dim,\n",
    "            n_embed=num_tokens,\n",
    "            # decay = vq_decay,\n",
    "            commitment=commitment_weight,\n",
    "        )\n",
    "\n",
    "        hdim = hidden_dim  # noqa\n",
    "\n",
    "        enc_chans = [hidden_dim] * num_layers\n",
    "        dec_chans = list(reversed(enc_chans))\n",
    "\n",
    "        enc_chans = [channels, *enc_chans]\n",
    "\n",
    "        dec_init_chan = codebook_dim if not has_resblocks else dec_chans[0]\n",
    "        dec_chans = [dec_init_chan, *dec_chans]\n",
    "\n",
    "        enc_chans_io, dec_chans_io = map(\n",
    "            lambda t: list(zip(t[:-1], t[1:])), (enc_chans, dec_chans)\n",
    "        )\n",
    "\n",
    "        enc_layers = []\n",
    "        dec_layers = []\n",
    "\n",
    "        for (enc_in, enc_out), (dec_in, dec_out) in zip(enc_chans_io, dec_chans_io):\n",
    "            enc_layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(enc_in, enc_out, 4, stride=2, padding=1), nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "            dec_layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(dec_in, dec_out, 4, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for _ in range(num_resnet_blocks):\n",
    "            dec_layers.insert(0, ResBlock(dec_chans[1]))\n",
    "            enc_layers.append(ResBlock(enc_chans[-1]))\n",
    "\n",
    "        if num_resnet_blocks > 0:\n",
    "            dec_layers.insert(0, nn.Conv2d(codebook_dim, dec_chans[1], 1))\n",
    "\n",
    "        enc_layers.append(nn.Conv2d(enc_chans[-1], codebook_dim, 1))\n",
    "        dec_layers.append(nn.Conv2d(dec_chans[-1], channels, 1))\n",
    "\n",
    "        self.encoder = nn.Sequential(*enc_layers)\n",
    "        self.decoder = nn.Sequential(*dec_layers)\n",
    "\n",
    "        self.loss_fn = F.smooth_l1_loss if smooth_l1_loss else F.mse_loss\n",
    "\n",
    "    def get_vq_commitment_loss(self, quantized, inputs):\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        c_loss = q_latent_loss + self.commitment_weight * e_latent_loss\n",
    "\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "\n",
    "        return quantized, c_loss\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        img,\n",
    "        return_loss=False,\n",
    "        return_recons=False,\n",
    "        return_encoded=False,\n",
    "    ):\n",
    "        shape, device = img.shape, img.device  # noqa\n",
    "\n",
    "        encoded = self.encoder(img)\n",
    "\n",
    "        if return_encoded:\n",
    "            return encoded\n",
    "\n",
    "        h, w = encoded.shape[-2:]\n",
    "\n",
    "        encoded = rearrange(encoded, \"b c h w -> b (h w) c\")\n",
    "        # quantized, _ = self.vq(encoded)\n",
    "        quantized = self.vq(encoded)\n",
    "\n",
    "        if self.use_vq_commit_loss:\n",
    "            quantized, c_loss = self.get_vq_commitment_loss(quantized, encoded)\n",
    "\n",
    "        quantized = rearrange(quantized, \"b (h w) c -> b c h w\", h=h, w=w)\n",
    "        out = self.decoder(quantized)\n",
    "\n",
    "        if not return_loss:\n",
    "            return out\n",
    "\n",
    "        # reconstruction loss and VQ commitment loss\n",
    "\n",
    "        recon_loss = self.loss_fn(img, out)\n",
    "\n",
    "        if self.use_vq_commit_loss:\n",
    "            loss = recon_loss + c_loss\n",
    "        else:\n",
    "            loss = recon_loss\n",
    "\n",
    "        if not return_recons:\n",
    "            return loss, recon_loss\n",
    "\n",
    "        return loss, recon_loss, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1fa0f-f0ac-4ad7-a855-1a93a00c9d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a928c76-4430-4adf-89ed-a2fbac48eedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace8668-9266-43ae-8ff4-10520ffa4554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e6218-3905-45dc-971b-3108209f34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class MaskedAutoencoderViT(nn.Module):\n",
    "    \"\"\"Masked Autoencoder with VisionTransformer backbone\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=256,\n",
    "        patch_size=8,\n",
    "        mask_ratio=0.75,\n",
    "        in_chans=3,\n",
    "        embed_dim=256,\n",
    "        depth=24,\n",
    "        num_heads=16,\n",
    "        decoder_embed_dim=128,\n",
    "        decoder_depth=8,\n",
    "        decoder_num_heads=16,\n",
    "        mlp_ratio=4.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        norm_pix_loss=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False\n",
    "        )  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    embed_dim,\n",
    "                    num_heads,\n",
    "                    mlp_ratio,\n",
    "                    qkv_bias=True,\n",
    "                    # qk_scale=None,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False\n",
    "        )  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    decoder_embed_dim,\n",
    "                    decoder_num_heads,\n",
    "                    mlp_ratio,\n",
    "                    qkv_bias=True,\n",
    "                    # qk_scale=None,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(decoder_depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(\n",
    "            decoder_embed_dim, patch_size**2 * in_chans, bias=True\n",
    "        )  # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(\n",
    "            self.pos_embed.shape[-1],\n",
    "            int(self.patch_embed.num_patches**0.5),\n",
    "            cls_token=True,\n",
    "        )\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(\n",
    "            self.decoder_pos_embed.shape[-1],\n",
    "            int(self.patch_embed.num_patches**0.5),\n",
    "            cls_token=True,\n",
    "        )\n",
    "        self.decoder_pos_embed.data.copy_(\n",
    "            torch.from_numpy(decoder_pos_embed).float().unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=0.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=0.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum(\"nchpwq->nhwpqc\", x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1] ** 0.5)\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - self.mask_ratio))\n",
    "\n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "\n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(\n",
    "            noise, dim=1\n",
    "        )  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_encoder_features(self, x):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(\n",
    "            x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1\n",
    "        )\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(\n",
    "            x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2])\n",
    "        )  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_decoder2(self, x):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove,\n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.0e-6) ** 0.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n",
    "\n",
    "    def forward2(self, imgs):\n",
    "        latent = self.forward_encoder_features(imgs)\n",
    "        pred = self.forward_decoder2(latent)  # [N, L, p*p*3]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313ec10-a426-43aa-9fc0-f332c87c4e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78169bd7-5520-4c35-ae95-c31921275ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8842f3a-b326-4f45-951d-1d410adfee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_mae_model(cnfg):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=cnfg.patch_size,\n",
    "        embed_dim=cnfg.encoder_embedding_dim,\n",
    "        depth=cnfg.depth_enc,\n",
    "        num_heads=cnfg.num_heads_enc,\n",
    "        decoder_embed_dim=cnfg.decoder_embedding_dim,\n",
    "        decoder_depth=cnfg.decoder_depth,\n",
    "        decoder_num_heads=cnfg.decoder_num_heads,\n",
    "        mlp_ratio=cnfg.mlp_ratio,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        img_size=cnfg.img_size,\n",
    "        in_chans=cnfg.in_chans,\n",
    "        mask_ratio=cnfg.mask_ratio,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d75f3a-930f-489e-b4a8-be7b150628c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf7686-d6e9-4dd2-b213-85eca16d69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_embeddings_mae(model, samples):\n",
    "    out = model.forward_encoder_features(samples)\n",
    "    cls_token = torch.select(out, 1, 0)\n",
    "\n",
    "    return cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b9f74f-edf0-4530-8d9d-13ee022ebe83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2648f2b-d419-46dd-a362-9ed4cdbd3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_vae_model(cnfg):\n",
    "    vae = MyVQVAE(\n",
    "        image_size=cnfg.img_size,\n",
    "        num_layers=cnfg.num_layers,  # number of downsamples - ex. 256 / (2 ** 3) = (32 x 32 feature map)\n",
    "        num_tokens=cnfg.num_tokens,  # number of visual tokens. in the paper, they used 8192, but could be smaller for downsized projects\n",
    "        codebook_dim=cnfg.codebook_dim,  # codebook dimension\n",
    "        hidden_dim=cnfg.hidden_dim,  # hidden dimension\n",
    "        num_resnet_blocks=cnfg.num_resnet_blocks,  # number of resnet blocks\n",
    "        use_vq_commit_loss=cnfg.use_vq_commit_loss,\n",
    "    )\n",
    "\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f791e6d2-df0d-4ac4-99c1-09b9aaacf229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_embeddings_vae(dvae_model, img):\n",
    "    encoded = dvae_model.encoder(img)\n",
    "\n",
    "    h, w = encoded.shape[-2:]\n",
    "\n",
    "    encoded = rearrange(encoded, \"b c h w -> b (h w) c\")\n",
    "\n",
    "    quantized = dvae_model.vq(encoded)\n",
    "\n",
    "    if dvae_model.use_vq_commit_loss:\n",
    "        quantized, c_loss = dvae_model.get_vq_commitment_loss(quantized, encoded)\n",
    "\n",
    "    quantized = rearrange(quantized, \"b (h w) c -> b c h w\", h=h, w=w)\n",
    "\n",
    "    return quantized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
