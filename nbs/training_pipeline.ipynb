{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5b57ee-04b3-488d-a84a-00a0c4b9e980",
   "metadata": {},
   "source": [
    "# Define: Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0954128-3599-43c5-85d7-c2780af3ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d8b2a-910d-4a34-b364-e037ec84ed29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a95d46-5753-42d0-945f-c1905385b2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e6365-a5e7-41fd-9a9d-4c836a7a71d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from colorama import Fore, Style\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from advanced_autoencoders.config import ConfigMaeLarge, ConfigVQVAE\n",
    "from advanced_autoencoders.dataset import MyImageDataset\n",
    "from advanced_autoencoders.trainers import (\n",
    "    PlModelMAE,\n",
    "    PlModelVQVAE,\n",
    "    get_model_checkpoint_callback,\n",
    "    get_trainer_mae,\n",
    "    get_trainer_vq,\n",
    ")\n",
    "from advanced_autoencoders.utils import (\n",
    "    get_train_transforms,\n",
    "    make_images_dataframe,\n",
    "    seed_everything,\n",
    ")\n",
    "\n",
    "r_ = Fore.RED\n",
    "b_ = Fore.BLUE\n",
    "c_ = Fore.CYAN\n",
    "g_ = Fore.GREEN\n",
    "y_ = Fore.YELLOW\n",
    "m_ = Fore.MAGENTA\n",
    "sr_ = Style.RESET_ALL\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64507607-652b-455f-be6a-ffee294330ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705ad39-2fa4-4d19-b16f-694fc89450a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def make_train_test_split(df: pd.DataFrame, cnfg):\n",
    "    df_train, df_val = train_test_split(df, test_size=cnfg.size_val, random_state=42)\n",
    "\n",
    "    return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0cc2f-f12e-44d8-af40-c69a7ef21ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_reconstruction_sample(cnfg, df):\n",
    "    val_reconstruction_dataset = MyImageDataset(\n",
    "        df, augmentations=get_train_transforms(cnfg)\n",
    "    )\n",
    "    val_dl_rec = DataLoader(val_reconstruction_dataset, batch_size=4, shuffle=False)\n",
    "    for test_sample in val_dl_rec:\n",
    "        break\n",
    "    test_sample = test_sample.cuda(f\"cuda:{cnfg.gpus[0]}\")\n",
    "\n",
    "    return test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5d7fa-0afc-4e01-83df-0d542cca4425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def train_pipeline_mae():\n",
    "    CONFIG = ConfigMaeLarge()\n",
    "    seed_everything(seed=CONFIG.seed)\n",
    "    df_all = make_images_dataframe(CONFIG)\n",
    "    print(f\"Making Image DF from DIR DONE! TOTAL IMAGES:{df_all.shape[0]}\")\n",
    "    df_train, df_val = make_train_test_split(df_all, CONFIG)\n",
    "    print(\n",
    "        f\"TRAIN VAL SPLIT DONE! TOTAL IMAGES TRAIN:{df_train.shape[0]}, VAL:{df_val.shape[0]}\"\n",
    "    )\n",
    "    test_sample = get_reconstruction_sample(CONFIG, df_val)\n",
    "    checkpoint_callback = get_model_checkpoint_callback(CONFIG)\n",
    "    trainer = get_trainer_mae(CONFIG, checkpoint_callback)\n",
    "    pl_model = PlModelMAE(CONFIG, df_train, df_val, test_sample)\n",
    "\n",
    "    trainer.fit(pl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba670555-a735-4e29-8d6a-215331bb88b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae312377-a55a-433e-a65a-89de38bbacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def train_pipeline_vqvae():\n",
    "    CONFIG = ConfigVQVAE()\n",
    "    seed_everything(seed=CONFIG.seed)\n",
    "    df_all = make_images_dataframe(CONFIG)\n",
    "    print(f\"Making Image DF from DIR DONE! TOTAL IMAGES:{df_all.shape[0]}\")\n",
    "    df_train, df_val = make_train_test_split(df_all, CONFIG)\n",
    "    print(\n",
    "        f\"TRAIN VAL SPLIT DONE! TOTAL IMAGES TRAIN:{df_train.shape[0]}, VAL:{df_val.shape[0]}\"\n",
    "    )\n",
    "    test_sample = get_reconstruction_sample(CONFIG, df_val)\n",
    "    checkpoint_callback = get_model_checkpoint_callback(CONFIG)\n",
    "    trainer = get_trainer_vq(CONFIG, checkpoint_callback)\n",
    "    pl_model = PlModelVQVAE(CONFIG, df_train, df_val, test_sample)\n",
    "\n",
    "    trainer.fit(pl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5962a1-694c-4da2-95d3-b29279b5dbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced3ea57-129d-4173-8db6-1ff30c6a0f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
