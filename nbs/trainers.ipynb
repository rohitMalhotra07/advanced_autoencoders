{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9931a8b-f1f4-4524-a736-50ac39cf1550",
   "metadata": {},
   "source": [
    "# Define Lightning based trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da8d54-cbe3-4604-8ad4-ef61c84e63ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae16b3-0d62-4cb2-9efc-4f8647f12cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f138d0-ada7-49d8-b527-14cdbdd3e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from colorama import Fore, Style\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from advanced_autoencoders.dataset import MyImageDataset\n",
    "from advanced_autoencoders.models import get_mae_model, get_vae_model\n",
    "from advanced_autoencoders.utils import get_test_transforms, get_train_transforms\n",
    "\n",
    "r_ = Fore.RED\n",
    "b_ = Fore.BLUE\n",
    "c_ = Fore.CYAN\n",
    "g_ = Fore.GREEN\n",
    "y_ = Fore.YELLOW\n",
    "m_ = Fore.MAGENTA\n",
    "sr_ = Style.RESET_ALL\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb616cf-8d2a-417c-8d85-170079a48419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e72c0-3021-4285-bd44-cff83e66a137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349569c-89d7-4438-9aeb-443f4a64f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def generate_and_save_images_vq(\n",
    "    dvae_model, epoch, test_sample, best_loss, cnfg, figsize=(20, 15)\n",
    "):\n",
    "    f, axarr = plt.subplots(1, 2, figsize=figsize)\n",
    "    img1 = torchvision.utils.make_grid(test_sample.cpu()).permute(1, 2, 0).numpy()\n",
    "    # axarr[0].imshow(img)\n",
    "\n",
    "    outputs = dvae_model(test_sample)\n",
    "    predictions = outputs.detach().cpu()\n",
    "    img2 = torchvision.utils.make_grid(predictions).permute(1, 2, 0).numpy()\n",
    "\n",
    "    axarr[0].imshow(img1)\n",
    "    axarr[1].imshow(img2)\n",
    "    axarr[0].set_title(\"Orignal\")\n",
    "    axarr[1].set_title(\"Reconstruction\")\n",
    "\n",
    "    if not os.path.exists(cnfg.TRAINING_RECON_IMG_DIR):\n",
    "        os.makedirs(cnfg.TRAINING_RECON_IMG_DIR)\n",
    "    plt.savefig(f\"{cnfg.TRAINING_RECON_IMG_DIR}image_{epoch}_{best_loss}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cbb6b8-df1c-4f97-b25e-d957c028f16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e897b-0472-481a-a0cb-06915aa8d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def generate_and_save_images_mae(\n",
    "    model, epoch, test_sample, best_loss, cnfg, figsize=(20, 15)\n",
    "):\n",
    "    f, axarr = plt.subplots(1, 2, figsize=figsize)\n",
    "    img1 = torchvision.utils.make_grid(test_sample.cpu()).permute(1, 2, 0).numpy()\n",
    "\n",
    "    loss, pred, mask = model(test_sample)\n",
    "    predictions = model.unpatchify(pred)\n",
    "    predictions = predictions.detach().cpu()\n",
    "    img2 = torchvision.utils.make_grid(predictions).permute(1, 2, 0).numpy()\n",
    "\n",
    "    axarr[0].imshow(img1)\n",
    "    axarr[1].imshow(img2)\n",
    "    axarr[0].set_title(\"Orignal\")\n",
    "    axarr[1].set_title(\"Reconstruction\")\n",
    "\n",
    "    if not os.path.exists(cnfg.TRAINING_RECON_IMG_DIR):\n",
    "        os.makedirs(cnfg.TRAINING_RECON_IMG_DIR)\n",
    "    plt.savefig(f\"{cnfg.TRAINING_RECON_IMG_DIR}image_{epoch}_{best_loss}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc53bc-c0b6-4224-b558-4a14058d2bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695049d-16b5-48c7-8db6-f504c17ac08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class PlModelVQVAE(pl.LightningModule):\n",
    "    def __init__(self, cnfg, df_train, df_val, test_sample):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = get_vae_model(cnfg)\n",
    "        # self.model.load_state_dict(torch.load(\"./\" + best_model_name + \".bin\"))\n",
    "\n",
    "        self.l_rate = cnfg.lr\n",
    "        self.batch_size = cnfg.bs\n",
    "        self.best_loss = 99999\n",
    "        self.epoch_train_loss = None\n",
    "        self.epoch_val_loss = None\n",
    "        self.epoch_val_rec_loss = None\n",
    "        self.validity_check_happened = False\n",
    "        self.cnfg = cnfg\n",
    "        self.test_sample = test_sample\n",
    "\n",
    "        self.df_train = df_train\n",
    "        self.df_val = df_val\n",
    "\n",
    "        self.validation_step_outs = []\n",
    "        self.train_step_outs = []\n",
    "\n",
    "    def forward(\n",
    "        self, inpt, return_loss=True, return_recons=False, return_encoded=False\n",
    "    ):\n",
    "        return self.model(\n",
    "            inpt,\n",
    "            return_loss=True,\n",
    "            return_recons=return_recons,\n",
    "            return_encoded=return_encoded,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # train\n",
    "        train_dataset = MyImageDataset(\n",
    "            self.df_train, augmentations=get_train_transforms(self.cnfg)\n",
    "        )\n",
    "        train_dl = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        return train_dl\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # valid\n",
    "        valid_dataset = MyImageDataset(\n",
    "            self.df_val, augmentations=get_train_transforms(self.cnfg)\n",
    "        )\n",
    "        valid_dl = DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        return valid_dl\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self.forward(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.train_step_outs.append({\"loss\": loss})\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        outputs = [k[\"loss\"].item() for k in self.train_step_outs]\n",
    "        self.epoch_train_loss = sum(outputs) / len(outputs)\n",
    "\n",
    "        self.train_step_outs = []\n",
    "\n",
    "        return\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_recon_loss = self.forward(batch)\n",
    "\n",
    "            self.log(\"val_loss\", val_loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "            self.log(\n",
    "                \"val_recon_loss\",\n",
    "                val_recon_loss,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "\n",
    "        self.validation_step_outs.append(\n",
    "            {\"val_loss\": val_loss, \"val_recon_loss\": val_recon_loss}\n",
    "        )\n",
    "        return {\"val_loss\": val_loss, \"val_recon_loss\": val_recon_loss}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.validity_check_happened:\n",
    "            val_losses = [k[\"val_loss\"].item() for k in self.validation_step_outs]\n",
    "            val_recon_losses = [\n",
    "                k[\"val_recon_loss\"].item() for k in self.validation_step_outs\n",
    "            ]\n",
    "            self.epoch_val_loss = sum(val_losses) / len(val_losses)\n",
    "            self.epoch_val_rec_loss = sum(val_recon_losses) / len(val_recon_losses)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch:{self.current_epoch} |Train Loss:{self.epoch_train_loss}|Valid Loss:{self.epoch_val_loss}|Rec Loss:{self.epoch_val_rec_loss}\"\n",
    "            )\n",
    "            if self.epoch_val_rec_loss <= self.best_loss:\n",
    "                print(\n",
    "                    f\"{g_}Rec Loss Decreased from {self.best_loss} to {self.epoch_val_rec_loss}{sr_}\"\n",
    "                )\n",
    "\n",
    "                self.best_loss = self.epoch_val_rec_loss\n",
    "                torch.save(\n",
    "                    self.model.state_dict(),\n",
    "                    f\"{self.cnfg.MODELS_DIR}{self.cnfg.model_name}.bin\",\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    generate_and_save_images_vq(\n",
    "                        self.model,\n",
    "                        self.current_epoch,\n",
    "                        self.test_sample,\n",
    "                        self.best_loss,\n",
    "                        self.cnfg,\n",
    "                    )\n",
    "        else:\n",
    "            self.validity_check_happened = True\n",
    "            print(\"Validity check done!\")\n",
    "\n",
    "        self.validation_step_outs = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a0231-b901-40c4-9e6b-75b139067359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec581b9-1c2a-4e69-9306-28941bcb4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class PlModelMAE(pl.LightningModule):\n",
    "    def __init__(self, cnfg, df_train, df_val, test_sample):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = get_mae_model(cnfg)\n",
    "        if cnfg.prev_model_name is not None:\n",
    "            self.model.load_state_dict(torch.load(\"./\" + cnfg.prev_model_name + \".bin\"))\n",
    "            print(\"Previous Model Loaded\")\n",
    "\n",
    "        self.best_loss = 99999\n",
    "        self.epoch_train_loss = None\n",
    "        self.epoch_val_loss = None\n",
    "        self.epoch_val_rec_loss = None\n",
    "        self.validity_check_happened = False\n",
    "        self.cnfg = cnfg\n",
    "        self.test_sample = test_sample\n",
    "        # self.set_test_sample = False\n",
    "\n",
    "        self.df_train = df_train\n",
    "        self.df_val = df_val\n",
    "\n",
    "        self.validation_step_outs = []\n",
    "        self.train_step_outs = []\n",
    "\n",
    "    def forward(self, inpt):\n",
    "        return self.model(inpt)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # train\n",
    "        train_dataset = MyImageDataset(\n",
    "            self.df_train, augmentations=get_train_transforms(self.cnfg)\n",
    "        )\n",
    "        train_dl = DataLoader(train_dataset, batch_size=self.cnfg.bs, shuffle=True)\n",
    "        return train_dl\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # valid\n",
    "        valid_dataset = MyImageDataset(\n",
    "            self.df_val, augmentations=get_test_transforms(self.cnfg)\n",
    "        )\n",
    "        valid_dl = DataLoader(valid_dataset, batch_size=self.cnfg.bs, shuffle=False)\n",
    "        return valid_dl\n",
    "\n",
    "    def training_step(self, x, batch_idx):\n",
    "        loss, pred, mask = self.forward(x)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        self.train_step_outs.append({\"loss\": loss})\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        outputs = [\n",
    "            k[\"loss\"].sum().item() / self.cnfg.num_gpus for k in self.train_step_outs\n",
    "        ]\n",
    "        self.epoch_train_loss = sum(outputs) / len(outputs)\n",
    "\n",
    "        self.train_step_outs = []\n",
    "        return\n",
    "\n",
    "    def validation_step(self, x, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            # if (( not self.set_test_sample) and (self.validity_check_happened)):\n",
    "            #     self.test_sample = x\n",
    "            #     self.set_test_sample = True\n",
    "\n",
    "            val_loss, pred, mask = self.forward(x)\n",
    "\n",
    "            self.log(\"val_loss\", val_loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        self.validation_step_outs.append({\"val_loss\": val_loss})\n",
    "        return {\"val_loss\": val_loss}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.validity_check_happened:\n",
    "            # print(validation_step_outputs)\n",
    "            val_losses = [\n",
    "                k[\"val_loss\"].sum().item() / self.cnfg.num_gpus\n",
    "                for k in self.validation_step_outs\n",
    "            ]\n",
    "\n",
    "            self.epoch_val_loss = sum(val_losses) / len(val_losses)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch:{self.current_epoch} |Train Loss:{self.epoch_train_loss}|Valid Loss:{self.epoch_val_loss}\"\n",
    "            )\n",
    "            if self.epoch_val_loss <= self.best_loss:\n",
    "                print(\n",
    "                    f\"{g_}Rec Loss Decreased from {self.best_loss} to {self.epoch_val_loss}{sr_}\"\n",
    "                )\n",
    "\n",
    "                self.best_loss = self.epoch_val_loss\n",
    "                torch.save(\n",
    "                    self.model.state_dict(),\n",
    "                    f\"{self.cnfg.MODELS_DIR}{self.cnfg.model_name}.bin\",\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    generate_and_save_images_mae(\n",
    "                        self.model,\n",
    "                        self.current_epoch,\n",
    "                        self.test_sample,\n",
    "                        self.best_loss,\n",
    "                        self.cnfg,\n",
    "                    )\n",
    "        else:\n",
    "            self.validity_check_happened = True\n",
    "            print(\"Validity check done!\")\n",
    "\n",
    "        self.validation_step_outs = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.cnfg.lr,\n",
    "            betas=self.cnfg.betas,\n",
    "            weight_decay=self.cnfg.weight_decay,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb8907-8b41-4c37-b5d4-0ff0dbaf8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_model_checkpoint_callback(cnfg):\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"train_loss\",\n",
    "        dirpath=cnfg.CHECKPOINTS_DIR + cnfg.model_name,\n",
    "        filename=cnfg.model_name + \"_{epoch}-{train_loss:.5f}-{val_loss:.5f}\",\n",
    "        save_top_k=2,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    return checkpoint_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f84596-1473-4e50-8263-67e41cbcf430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9f106-fbba-4559-8d3b-b6d54e8fec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_trainer_vq(cnfg, checkpoint_callback):\n",
    "    trainer = Trainer(\n",
    "        max_epochs=cnfg.epochs,\n",
    "        fast_dev_run=False,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        default_root_dir=cnfg.TRAINING_LOGS_DIR,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866b557-7d51-4ffd-bd6f-6b37ea1e8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_trainer_mae(cnfg, checkpoint_callback):\n",
    "    trainer = Trainer(\n",
    "        max_epochs=cnfg.epochs,\n",
    "        fast_dev_run=False,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        # strategy=\"dp\",\n",
    "        accelerator=\"gpu\",\n",
    "        # gpus=cnfg.gpus,\n",
    "        devices=1,\n",
    "        default_root_dir=cnfg.TRAINING_LOGS_DIR,\n",
    "        # num_sanity_val_steps=0\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
